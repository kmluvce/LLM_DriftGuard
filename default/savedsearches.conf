[LLM Baseline Calculator]
search = index=llm_logs earliest=-30d@d latest=-1d@d \
| stats avg(response_time) as avg_response_time, \
        avg(token_count) as avg_token_count, \
        avg(confidence_score) as avg_confidence, \
        stdev(response_time) as std_response_time, \
        stdev(token_count) as std_token_count, \
        stdev(confidence_score) as std_confidence by model_id \
| outputlookup model_baselines.csv
description = Calculate baseline metrics for all LLM models using 30-day historical data
cron_schedule = 0 2 * * *
dispatch.earliest_time = -30d@d
dispatch.latest_time = -1d@d
is_scheduled = 1
enableSched = 1

[LLM High Drift Alert]
search = index=llm_logs earliest=-1h@h latest=now \
| driftdetect field=response baseline_file="model_baselines.csv" threshold=0.7 \
| where drift_score > 0.5 \
| stats count as high_drift_events, avg(drift_score) as avg_drift by model_id \
| where high_drift_events > 5
description = Alert when high semantic drift is detected in LLM responses
cron_schedule = */15 * * * *
dispatch.earliest_time = -1h@h
dispatch.latest_time = now
is_scheduled = 1
enableSched = 1
alert.track = 1
alert.severity = 3
action.email = 1
action.email.to = admin@company.com
action.email.subject = LLM DriftGuard: High Semantic Drift Detected
action.email.message = High semantic drift detected in model $result.model_id$. \
Average drift score: $result.avg_drift$. Number of high drift events: $result.high_drift_events$.

[LLM Performance Degradation Alert]
search = index=llm_logs earliest=-1h@h latest=now \
| stats avg(response_time) as current_avg_time by model_id \
| lookup model_baselines.csv model_id OUTPUT avg_response_time as baseline_time \
| eval pct_increase = ((current_avg_time - baseline_time) / baseline_time) * 100 \
| where pct_increase > 50
description = Alert when LLM response times increase significantly above baseline
cron_schedule = */30 * * * *
dispatch.earliest_time = -1h@h
dispatch.latest_time = now
is_scheduled = 1
enableSched = 1
alert.track = 1
alert.severity = 2
action.email = 1
action.email.to = admin@company.com
action.email.subject = LLM DriftGuard: Performance Degradation Alert
action.email.message = Performance degradation detected in model $result.model_id$. \
Response time increased by $result.pct_increase$% above baseline.

[LLM Low Confidence Alert]
search = index=llm_logs earliest=-30m@m latest=now \
| where confidence_score < 0.3 \
| stats count as low_confidence_count by model_id \
| where low_confidence_count > 10
description = Alert when LLM confidence scores drop below acceptable levels
cron_schedule = */15 * * * *
dispatch.earliest_time = -30m@m
dispatch.latest_time = now
is_scheduled = 1
enableSched = 1
alert.track = 1
alert.severity = 2
action.email = 1
action.email.to = admin@company.com
action.email.subject = LLM DriftGuard: Low Confidence Alert
action.email.message = Low confidence responses detected in model $result.model_id$. \
Count of low confidence responses: $result.low_confidence_count$.

[LLM Anomaly Detection Alert]
search = index=llm_logs earliest=-2h@h latest=now \
| anomalydetect fields="response_time,token_count,confidence_score" method=zscore threshold=3.0 \
| where anomaly_detected=true \
| stats count as anomaly_count by model_id, anomaly_type \
| where anomaly_count > 3
description = Alert when statistical anomalies are detected in LLM behavior
cron_schedule = */20 * * * *
dispatch.earliest_time = -2h@h
dispatch.latest_time = now
is_scheduled = 1
enableSched = 1
alert.track = 1
alert.severity = 2
action.email = 1
action.email.to = admin@company.com
action.email.subject = LLM DriftGuard: Anomaly Detection Alert
action.email.message = Anomalies detected in model $result.model_id$ of type $result.anomaly_type$. \
Number of anomalies: $result.anomaly_count$.

[LLM Daily Summary Report]
search = index=llm_logs earliest=-1d@d latest=@d \
| eval date = strftime(_time, "%Y-%m-%d") \
| stats count as total_requests, \
        avg(response_time) as avg_response_time, \
        avg(confidence_score) as avg_confidence, \
        avg(token_count) as avg_tokens, \
        min(response_time) as min_response_time, \
        max(response_time) as max_response_time by date, model_id \
| driftdetect field=response baseline_file="model_baselines.csv" \
| stats avg(drift_score) as avg_drift by date, model_id \
| eval report_generated = now() \
| outputlookup append=true daily_llm_summary.csv
description = Generate daily summary report of LLM performance metrics
cron_schedule = 0 1 * * *
dispatch.earliest_time = -1d@d
dispatch.latest_time = @d
is_scheduled = 1
enableSched = 1

[LLM Weekly Trend Analysis]
search = index=llm_logs earliest=-7d@d latest=now \
| bucket _time span=1d \
| eval day = strftime(_time, "%Y-%m-%d") \
| stats avg(response_time) as daily_avg_time, \
        avg(confidence_score) as daily_avg_confidence, \
        count as daily_requests by day, model_id \
| sort day \
| streamstats current=true window=7 avg(daily_avg_time) as weekly_trend_time, \
             avg(daily_avg_confidence) as weekly_trend_confidence by model_id \
| eval time_trend = case( \
    daily_avg_time > weekly_trend_time * 1.1, "increasing", \
    daily_avg_time < weekly_trend_time * 0.9, "decreasing", \
    1=1, "stable") \
| eval confidence_trend = case( \
    daily_avg_confidence > weekly_trend_confidence * 1.05, "improving", \
    daily_avg_confidence < weekly_trend_confidence * 0.95, "declining", \
    1=1, "stable") \
| where day = strftime(now(), "%Y-%m-%d") \
| table model_id, time_trend, confidence_trend, daily_avg_time, daily_avg_confidence
description = Analyze weekly trends in LLM performance metrics
cron_schedule = 0 8 * * 1
dispatch.earliest_time = -7d@d
dispatch.latest_time = now
is_scheduled = 1
enableSched = 1

[LLM Error Pattern Detection]
search = index=llm_logs earliest=-1h@h latest=now \
| where isnotnull(error) OR response_time > 30 OR confidence_score < 0.1 \
| eval error_category = case( \
    isnotnull(error), "api_error", \
    response_time > 30, "timeout", \
    confidence_score < 0.1, "low_confidence", \
    1=1, "other") \
| stats count as error_count, \
        list(error) as error_messages by model_id, error_category \
| where error_count > 5
description = Detect patterns in LLM errors and failures
cron_schedule = */10 * * * *
dispatch.earliest_time = -1h@h
dispatch.latest_time = now
is_scheduled = 1
enableSched = 1
alert.track = 1
alert.severity = 1
action.email = 1
action.email.to = admin@company.com
action.email.subject = LLM DriftGuard: Error Pattern Alert
action.email.message = Error pattern detected in model $result.model_id$. \
Error category: $result.error_category$, Count: $result.error_count$.
